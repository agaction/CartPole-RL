{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process\n",
    "Reinforcement learning is based on the idea of the *agent* and the *environment.* After each action, the agent makes some observations about their new state in the environment, and receives an award, which roughly indicates how \"good\" the current state is. The agent's goal is to take the series of actions that will maximize the overall cumulative reward. Reinforcement learning is a mathematical approach to solving problems modeled in this way.\n",
    "\n",
    "Our goal is to model a function $f$ that, given any state $s$ from our state space, we get the optimal action $a$ to maximize our overall cumulative reward. In other words, we want the function that gives $f(s) = a$. In most cases, the state space is too large to simply determine and remember the optimal move in every possible state, so we must try other approaches. Markov Decision Processes (MDPs) are one such approach. Some of the formal language is denoted below. \n",
    "\n",
    "$a$ := action  \n",
    "$s$ := initial state  \n",
    "$s'$ := next state  \n",
    "$o$ := observation\n",
    "$r_a(s, s')$ := reward from taking action $a$ to move from $s$ to $s'$ \n",
    "$\\mu$ := deterministic policy function, *i.e.* $\\mu(s_t) = a_t$  \n",
    "$\\pi$ := stochastic policy function, which gives a probability distribution for each potential action $a$ given the current state $s$.  \n",
    "$\\gamma$ := discount factor, which is used to weight the foreseen rewards of actions past the initial one\n",
    "\n",
    "**NOTE:** \"Reinforcement learning notation sometimes puts the symbol for state, $s$, in places where it would be technically more appropriate to write the symbol for observation, $o$. Specifically, this happens when talking about how the agent decides an action: we often signal in notation that the action is conditioned on the state, when in practice, the action is conditioned on the observation because the agent does not have access to the state. Also, \"because the policy is essentially the agent’s brain, it’s not uncommon to substitute the word “policy” for “agent” [(openai RL-intro)](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html).\"\n",
    "\n",
    "Now, we define our cumulative reward $R$ as $$R = \\sum_{t=0}^{\\infty} \\gamma^t r_{a_t}(s_t, s_{t+1}),$$ which the policy function $\\pi$ seeks to optimize. Note that this cumulative reward is simply a weighted sum of the rewards from each individual action over a period of time. To solve this MDP, we often need heuristics to assign suitable rewards, which can be difficult to determine, like for game of chess. For this reason, we introduce the state value function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
